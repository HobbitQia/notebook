# Paper

!!! Abstract
    这里用来放读论文的笔记，写的不好还请见谅>_<|||

* [BiS-KM: Enabling Any-Precision K-Means on FPGAs](BiS-KM.md)
* [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](ZeRO.md)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](Megatron-LM.md)
* [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](GPipe.md)
* [Mixed Precision Training](mixed_precision.md)
* [FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding](FAST.md)
* [Softmax Acceleration with Adaptive Numeric Format for both Training and Inference](softmax.md)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention](PagedAttention.md)
* [VecPAC: A Vectorizable and Precision-Aware CGRA](VecPAC.md)
* [NN-LUT: Neural Approximation of Non-Linear Operations for Efficient Transformer Inference](NN-LUT.md)
* [ChatEDA: A Large Language Model Powered Autonomous Agent for EDA](ChatEDA.md)
* [Improving Language Understanding by Generative Pre-Training](GPT-1.md)
* [Language Models are Unsupervised Multitask Learners](GPT-2.md)
* [Language Models are Few-Shot Learners](GPT-3.md)
* [APEX: A Framework for Automated Processing Element Design Space Exploration using Frequent Subgraph Analysis](APEX.md)